#!/usr/bin/env python3
"""
Test Script for Investigation-Insight Cross-Module Intelligence - Phase 2.3 Validation
Demonstrates bidirectional learning and pattern discovery between Investigation and Insight Synthesis.
"""

import asyncio
import time
from pathlib import Path
import sys
from datetime import datetime, timezone, timedelta
import uuid
import json

# Add current directory to path
sys.path.insert(0, '.')

from investigation_insight_intelligence import (
    InvestigationInsightIntelligenceEngine,
    InvestigationInsightLink,
    InvestigationInsightLinkType,
    CrossModulePattern,
    FeedbackLoop,
    FeedbackLoopType,
    InvestigationInsightIntelligence,
    analyze_investigation_insight_intelligence
)

# Check available components
try:
    from enterprise_vector_schema import ModuleSource
    VECTOR_INFRASTRUCTURE = True
except ImportError:
    print("‚ö†Ô∏è Vector infrastructure not available")
    VECTOR_INFRASTRUCTURE = False
    # Fallback
    from enum import Enum
    class ModuleSource(Enum):
        INVESTIGATION = "investigation"
        INSIGHT_SYNTHESIS = "insight_synthesis"


async def test_engine_initialization():
    """Test basic engine initialization."""
    print("üß™ Testing Engine Initialization")
    print("=" * 55)
    
    try:
        # Initialize engine
        engine = InvestigationInsightIntelligenceEngine()
        print(f"‚úÖ Engine created successfully")
        print(f"üìä Logger initialized: {engine.logger is not None}")
        print(f"üìä Link cache ready: {isinstance(engine.link_cache, dict)}")
        print(f"üìä Pattern cache ready: {isinstance(engine.pattern_cache, dict)}")
        print(f"üìä Feedback loops ready: {isinstance(engine.feedback_loops, dict)}")
        
        # Try vector initialization
        db_path = Path(__file__).parent.parent / "data"
        await engine.initialize(db_path=str(db_path))
        
        print(f"\nüìä Vector initialization:")
        print(f"   - Embedder: {engine.embedder is not None}")
        print(f"   - Vector DB: {engine.vector_db is not None}")
        print(f"   - Vector table: {engine.vector_table is not None}")
        
        if not engine.embedder:
            print("   ‚ö†Ô∏è Vector capabilities not available - this is expected without dependencies")
        
        await engine.cleanup()
        print("\n‚úÖ Engine initialization test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Engine initialization test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_investigation_insight_linking():
    """Test linking investigations to insights."""
    print("\n" + "=" * 55)
    print("üß™ Testing Investigation-Insight Linking")
    print("=" * 55)
    
    try:
        engine = InvestigationInsightIntelligenceEngine()
        
        # Create mock investigation-insight link
        mock_link = InvestigationInsightLink(
            link_id=str(uuid.uuid4()),
            link_type=InvestigationInsightLinkType.DIRECT_GENERATION,
            investigation_id="inv-test-001",
            insight_ids=["ins-test-001", "ins-test-002"],
            correlation_strength=0.85,
            confidence_score=0.9,
            business_impact=0.75,
            shared_patterns=["efficiency_analysis", "cost_optimization"],
            domain_overlap=["operations", "finance"],
            temporal_distance_hours=0.5,
            insight_quality_improvement=0.2,
            investigation_efficiency_gain=0.15,
            cross_validation_score=0.88,
            link_created=datetime.now(timezone.utc),
            last_updated=datetime.now(timezone.utc)
        )
        
        # Add to cache
        engine.link_cache[mock_link.link_id] = mock_link
        
        print(f"‚úÖ Created investigation-insight link")
        print(f"üìä Link type: {mock_link.link_type.value}")
        print(f"üìä Correlation strength: {mock_link.correlation_strength:.3f}")
        print(f"üìä Business impact: {mock_link.business_impact:.3f}")
        print(f"üìä Shared patterns: {', '.join(mock_link.shared_patterns)}")
        print(f"üìä Domain overlap: {', '.join(mock_link.domain_overlap)}")
        print(f"üìä Quality improvement: {mock_link.insight_quality_improvement:.3f}")
        
        # Test analyzing investigation flow (mock)
        print("\nüîç Testing investigation flow analysis...")
        
        # Simulate flow analysis
        links = await engine.analyze_investigation_to_insight_flow(
            "inv-test-001", time_window_hours=24
        )
        
        print(f"üìä Flow analysis completed")
        print(f"üìä Links in cache: {len(engine.link_cache)}")
        
        # Create more links for pattern testing
        for i in range(3):
            link = InvestigationInsightLink(
                link_id=str(uuid.uuid4()),
                link_type=InvestigationInsightLinkType.PATTERN_CORRELATION,
                investigation_id=f"inv-test-{i+2:03d}",
                insight_ids=[f"ins-test-{i+3:03d}"],
                correlation_strength=0.7 + i * 0.05,
                confidence_score=0.8,
                business_impact=0.6 + i * 0.1,
                shared_patterns=["pattern_a", "pattern_b"],
                domain_overlap=["operations"],
                temporal_distance_hours=1.0 + i,
                insight_quality_improvement=0.1 + i * 0.05,
                investigation_efficiency_gain=0.1,
                cross_validation_score=0.8,
                link_created=datetime.now(timezone.utc),
                last_updated=datetime.now(timezone.utc)
            )
            engine.link_cache[link.link_id] = link
        
        print(f"üìä Total links created: {len(engine.link_cache)}")
        
        await engine.cleanup()
        print("\n‚úÖ Investigation-insight linking test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Linking test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_pattern_discovery():
    """Test cross-module pattern discovery."""
    print("\n" + "=" * 55)
    print("üß™ Testing Cross-Module Pattern Discovery")
    print("=" * 55)
    
    try:
        engine = InvestigationInsightIntelligenceEngine()
        
        # Create mock patterns
        patterns = [
            CrossModulePattern(
                pattern_id=str(uuid.uuid4()),
                pattern_name="Efficiency_Quality_Correlation",
                pattern_description="Production efficiency drops correlate with quality issues leading to customer insights",
                occurrence_count=15,
                avg_correlation_strength=0.82,
                business_domains=["production", "quality", "customer"],
                avg_insight_quality=0.85,
                avg_investigation_confidence=0.88,
                business_value_generated=0.9,
                prediction_accuracy=0.83,
                recommendation_success_rate=0.78,
                example_investigations=["inv-001", "inv-002", "inv-003"],
                example_insights=["ins-001", "ins-002", "ins-003"],
                first_observed=datetime.now(timezone.utc) - timedelta(days=30),
                last_observed=datetime.now(timezone.utc),
                trend="increasing"
            ),
            CrossModulePattern(
                pattern_id=str(uuid.uuid4()),
                pattern_name="Supply_Chain_Financial_Impact",
                pattern_description="Supply chain investigations reveal financial impact insights",
                occurrence_count=8,
                avg_correlation_strength=0.75,
                business_domains=["supply_chain", "finance"],
                avg_insight_quality=0.8,
                avg_investigation_confidence=0.85,
                business_value_generated=0.85,
                prediction_accuracy=0.8,
                recommendation_success_rate=0.72,
                example_investigations=["inv-004", "inv-005"],
                example_insights=["ins-004", "ins-005"],
                first_observed=datetime.now(timezone.utc) - timedelta(days=20),
                last_observed=datetime.now(timezone.utc),
                trend="stable"
            ),
            CrossModulePattern(
                pattern_id=str(uuid.uuid4()),
                pattern_name="Customer_Operations_Feedback",
                pattern_description="Customer feedback investigations drive operational insights",
                occurrence_count=12,
                avg_correlation_strength=0.78,
                business_domains=["customer", "operations"],
                avg_insight_quality=0.82,
                avg_investigation_confidence=0.86,
                business_value_generated=0.88,
                prediction_accuracy=0.81,
                recommendation_success_rate=0.75,
                example_investigations=["inv-006", "inv-007", "inv-008"],
                example_insights=["ins-006", "ins-007", "ins-008"],
                first_observed=datetime.now(timezone.utc) - timedelta(days=25),
                last_observed=datetime.now(timezone.utc),
                trend="increasing"
            )
        ]
        
        # Add patterns to cache
        for pattern in patterns:
            engine.pattern_cache[pattern.pattern_id] = pattern
        
        print(f"‚úÖ Created {len(patterns)} cross-module patterns")
        
        # Display pattern details
        for i, pattern in enumerate(patterns):
            print(f"\nüìä Pattern {i+1}: {pattern.pattern_name}")
            print(f"   - Description: {pattern.pattern_description[:60]}...")
            print(f"   - Occurrences: {pattern.occurrence_count}")
            print(f"   - Correlation: {pattern.avg_correlation_strength:.3f}")
            print(f"   - Domains: {', '.join(pattern.business_domains)}")
            print(f"   - Business value: {pattern.business_value_generated:.3f}")
            print(f"   - Prediction accuracy: {pattern.prediction_accuracy:.3f}")
            print(f"   - Trend: {pattern.trend}")
        
        # Test pattern discovery (mock)
        print("\nüîç Testing pattern discovery process...")
        discovered_patterns = await engine.discover_cross_module_patterns(
            min_occurrences=5,
            time_window_days=30
        )
        
        print(f"üìä Pattern discovery completed")
        print(f"üìä Total patterns in cache: {len(engine.pattern_cache)}")
        
        # Analyze pattern value
        total_value = sum(p.business_value_generated for p in engine.pattern_cache.values())
        avg_accuracy = sum(p.prediction_accuracy for p in engine.pattern_cache.values()) / len(engine.pattern_cache) if engine.pattern_cache else 0
        
        print(f"\nüìä Pattern analytics:")
        print(f"   - Total business value: {total_value:.2f}")
        print(f"   - Average prediction accuracy: {avg_accuracy:.3f}")
        print(f"   - High-value patterns: {len([p for p in engine.pattern_cache.values() if p.business_value_generated > 0.85])}")
        
        await engine.cleanup()
        print("\n‚úÖ Pattern discovery test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Pattern discovery test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_feedback_loops():
    """Test feedback loop establishment and tracking."""
    print("\n" + "=" * 55)
    print("üß™ Testing Feedback Loop Management")
    print("=" * 55)
    
    try:
        engine = InvestigationInsightIntelligenceEngine()
        
        # Create mock feedback loops
        loops = [
            FeedbackLoop(
                loop_id=str(uuid.uuid4()),
                loop_type=FeedbackLoopType.INSIGHT_QUALITY,
                strength=0.85,
                iterations_count=5,
                improvement_rate=0.04,  # 4% per iteration
                convergence_speed=0.8,
                investigation_improvement=0.15,
                insight_improvement=0.2,
                overall_effectiveness=0.82,
                active_investigations=["inv-001", "inv-002", "inv-003"],
                active_insights=["ins-001", "ins-002", "ins-003", "ins-004"],
                is_active=True,
                last_activation=datetime.now(timezone.utc)
            ),
            FeedbackLoop(
                loop_id=str(uuid.uuid4()),
                loop_type=FeedbackLoopType.PATTERN_DISCOVERY,
                strength=0.75,
                iterations_count=3,
                improvement_rate=0.06,
                convergence_speed=0.6,
                investigation_improvement=0.18,
                insight_improvement=0.12,
                overall_effectiveness=0.78,
                active_investigations=["inv-004", "inv-005"],
                active_insights=["ins-005", "ins-006"],
                is_active=True,
                last_activation=datetime.now(timezone.utc) - timedelta(hours=2)
            )
        ]
        
        # Add loops to engine
        for loop in loops:
            engine.feedback_loops[loop.loop_id] = loop
        
        print(f"‚úÖ Created {len(loops)} feedback loops")
        
        # Display loop details
        for i, loop in enumerate(loops):
            print(f"\nüìä Feedback Loop {i+1}:")
            print(f"   - Type: {loop.loop_type.value}")
            print(f"   - Strength: {loop.strength:.3f}")
            print(f"   - Iterations: {loop.iterations_count}")
            print(f"   - Improvement rate: {loop.improvement_rate:.2%}")
            print(f"   - Convergence speed: {loop.convergence_speed:.3f}")
            print(f"   - Investigation improvement: {loop.investigation_improvement:.3f}")
            print(f"   - Insight improvement: {loop.insight_improvement:.3f}")
            print(f"   - Overall effectiveness: {loop.overall_effectiveness:.3f}")
        
        # Test feedback loop establishment
        print("\nüîç Testing feedback loop establishment...")
        new_loops = await engine.establish_feedback_loops(min_iterations=2)
        
        print(f"üìä Feedback loop analysis completed")
        print(f"üìä Total active loops: {len([l for l in engine.feedback_loops.values() if l.is_active])}")
        
        # Analyze loop effectiveness
        avg_effectiveness = sum(l.overall_effectiveness for l in engine.feedback_loops.values()) / len(engine.feedback_loops) if engine.feedback_loops else 0
        total_improvement = sum(l.investigation_improvement + l.insight_improvement for l in engine.feedback_loops.values())
        
        print(f"\nüìä Feedback loop analytics:")
        print(f"   - Average effectiveness: {avg_effectiveness:.3f}")
        print(f"   - Total improvement: {total_improvement:.3f}")
        print(f"   - Active investigations: {len(set(inv for l in engine.feedback_loops.values() for inv in l.active_investigations))}")
        
        await engine.cleanup()
        print("\n‚úÖ Feedback loop test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Feedback loop test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_predictive_capabilities():
    """Test predictive capabilities for insight quality."""
    print("\n" + "=" * 55)
    print("üß™ Testing Predictive Capabilities")
    print("=" * 55)
    
    try:
        engine = InvestigationInsightIntelligenceEngine()
        
        # Mock investigation results
        investigation_results = {
            "investigation_id": "inv-predict-001",
            "overall_confidence": 0.88,
            "total_duration_seconds": 180,
            "investigation_findings": {
                "key_findings": [
                    "Production efficiency dropped 20%",
                    "Quality defects increased to 3.5%",
                    "Customer complaints up 15%"
                ]
            },
            "business_context": {
                "domain": "production",
                "complexity_level": "high",
                "impact_level": "critical"
            }
        }
        
        print("üìä Test investigation:")
        print(f"   - ID: {investigation_results['investigation_id']}")
        print(f"   - Confidence: {investigation_results['overall_confidence']:.3f}")
        print(f"   - Domain: {investigation_results['business_context']['domain']}")
        print(f"   - Findings: {len(investigation_results['investigation_findings']['key_findings'])}")
        
        # Predict insight quality
        print("\nüîÆ Predicting insight quality...")
        predictions = await engine.predict_insight_quality(investigation_results)
        
        print(f"\nüìä Quality predictions:")
        for metric, value in predictions.items():
            if metric != "error":
                print(f"   - {metric}: {value:.3f}")
        
        # Test recommendation generation
        print("\nüîç Testing investigation area recommendations...")
        
        # Mock current insights
        current_insights = [
            {
                "type": "operational",
                "business_domain": "production",
                "title": "Production efficiency insight"
            },
            {
                "type": "strategic", 
                "business_domain": "customer",
                "title": "Customer satisfaction insight"
            }
        ]
        
        business_context = {
            "domain": "operations",
            "strategic_goal": "Improve operational efficiency by 15%",
            "current_initiative": "Operational Excellence 2024"
        }
        
        recommendations = await engine.recommend_investigation_areas(
            current_insights, business_context
        )
        
        print(f"\nüìä Recommended investigation areas ({len(recommendations)}):")
        for i, rec in enumerate(recommendations[:5]):
            print(f"\n   {i+1}. {rec['investigation_area']}")
            print(f"      - Priority: {rec['priority_score']:.3f}")
            print(f"      - Expected value: {rec['expected_value']:.3f}")
            print(f"      - Confidence: {rec['confidence']:.3f}")
            print(f"      - Duration: {rec['estimated_duration']}")
            print(f"      - Rationale: {rec['rationale']}")
        
        await engine.cleanup()
        print("\n‚úÖ Predictive capabilities test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Predictive test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_pipeline_optimization():
    """Test pipeline optimization recommendations."""
    print("\n" + "=" * 55)
    print("üß™ Testing Pipeline Optimization")
    print("=" * 55)
    
    try:
        engine = InvestigationInsightIntelligenceEngine()
        
        # Set up test data for optimization analysis
        # Add links with varying performance
        for i in range(5):
            link = InvestigationInsightLink(
                link_id=str(uuid.uuid4()),
                link_type=InvestigationInsightLinkType.DIRECT_GENERATION,
                investigation_id=f"inv-opt-{i:03d}",
                insight_ids=[f"ins-opt-{i:03d}"],
                correlation_strength=0.6 + i * 0.08,
                confidence_score=0.7 + i * 0.05,
                business_impact=0.5 + i * 0.1,
                shared_patterns=["optimization_test"],
                domain_overlap=["operations"],
                temporal_distance_hours=0.5 + i * 0.5,  # Increasing latency
                insight_quality_improvement=0.05 + i * 0.03,
                investigation_efficiency_gain=0.02 + i * 0.02,
                cross_validation_score=0.75,
                link_created=datetime.now(timezone.utc),
                last_updated=datetime.now(timezone.utc)
            )
            engine.link_cache[link.link_id] = link
        
        # Add a pattern for utilization testing
        pattern = CrossModulePattern(
            pattern_id=str(uuid.uuid4()),
            pattern_name="Underutilized_Pattern",
            pattern_description="A valuable pattern that's not being fully utilized",
            occurrence_count=2,  # Low utilization
            avg_correlation_strength=0.85,
            business_domains=["finance", "hr"],  # Domains not in links
            avg_insight_quality=0.9,
            avg_investigation_confidence=0.88,
            business_value_generated=0.95,
            prediction_accuracy=0.85,
            recommendation_success_rate=0.8,
            example_investigations=["inv-unused-001"],
            example_insights=["ins-unused-001"],
            first_observed=datetime.now(timezone.utc) - timedelta(days=10),
            last_observed=datetime.now(timezone.utc),
            trend="stable"
        )
        engine.pattern_cache[pattern.pattern_id] = pattern
        
        print(f"üìä Test setup:")
        print(f"   - Links created: {len(engine.link_cache)}")
        print(f"   - Patterns available: {len(engine.pattern_cache)}")
        print(f"   - Average time to insight: {sum(l.temporal_distance_hours for l in engine.link_cache.values()) / len(engine.link_cache):.2f} hours")
        
        # Get optimization recommendations
        print("\nüîß Analyzing pipeline for optimizations...")
        optimizations = await engine.optimize_investigation_insight_pipeline(
            performance_threshold=0.8
        )
        
        print(f"\nüìä Optimization opportunities found: {len(optimizations)}")
        for i, opt in enumerate(optimizations):
            print(f"\n   {i+1}. {opt['area']}:")
            print(f"      - Type: {opt['type']}")
            print(f"      - Current: {opt['current_value']:.3f}")
            print(f"      - Target: {opt['target_value']:.3f}")
            print(f"      - Recommendation: {opt['recommendation']}")
            print(f"      - Expected improvement: {opt['expected_improvement']:.2%}")
            print(f"      - Implementation effort: {opt['implementation_effort']}")
        
        await engine.cleanup()
        print("\n‚úÖ Pipeline optimization test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Pipeline optimization test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_comprehensive_intelligence_report():
    """Test comprehensive intelligence report generation."""
    print("\n" + "=" * 55)
    print("üß™ Testing Comprehensive Intelligence Report")
    print("=" * 55)
    
    try:
        # Use high-level interface
        print("üîç Generating cross-module intelligence report...")
        
        start_time = time.perf_counter()
        report = await analyze_investigation_insight_intelligence(
            time_window_days=30
        )
        generation_time = (time.perf_counter() - start_time) * 1000
        
        print(f"\n‚úÖ Intelligence report generated in {generation_time:.1f}ms")
        print(f"\nüìä Report Summary:")
        print(f"   - Report ID: {report.intelligence_id}")
        print(f"   - Analysis timestamp: {report.analysis_timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"\nüìä Link Analysis:")
        print(f"   - Active links: {len(report.active_links)}")
        print(f"   - Avg investigation-to-insight time: {report.avg_investigation_to_insight_time/3600:.2f} hours")
        print(f"   - Insight generation success rate: {report.insight_generation_success_rate:.2%}")
        
        if report.active_links:
            print(f"\n   Top links by correlation:")
            for link in sorted(report.active_links, key=lambda x: x.correlation_strength, reverse=True)[:3]:
                print(f"   - {link.investigation_id} ‚Üí {', '.join(link.insight_ids[:2])}")
                print(f"     Correlation: {link.correlation_strength:.3f}, Impact: {link.business_impact:.3f}")
        
        print(f"\nüìä Pattern Discovery:")
        print(f"   - Discovered patterns: {len(report.discovered_patterns)}")
        print(f"   - Pattern discovery rate: {report.pattern_discovery_rate:.2%}")
        
        if report.discovered_patterns:
            print(f"\n   Top patterns by value:")
            for pattern in sorted(report.discovered_patterns, key=lambda x: x.business_value_generated, reverse=True)[:3]:
                print(f"   - {pattern.pattern_name}")
                print(f"     Occurrences: {pattern.occurrence_count}, Value: {pattern.business_value_generated:.3f}")
                print(f"     Domains: {', '.join(pattern.business_domains)}")
        
        print(f"\nüìä Feedback Loops:")
        print(f"   - Active feedback loops: {len(report.feedback_loops)}")
        
        if report.feedback_loops:
            print(f"\n   Loop types:")
            for loop in report.feedback_loops:
                print(f"   - {loop.loop_type.value}: Strength={loop.strength:.3f}, Effectiveness={loop.overall_effectiveness:.3f}")
        
        print(f"\nüìä Quality Metrics:")
        print(f"   - Avg insight quality: {report.avg_insight_quality_score:.3f}")
        print(f"   - Avg investigation confidence: {report.avg_investigation_confidence:.3f}")
        print(f"   - Cross-validation accuracy: {report.cross_validation_accuracy:.3f}")
        
        print(f"\nüìä Business Impact:")
        print(f"   - Total business value: {report.total_business_value:.2f}")
        print(f"   - ROI multiplier: {report.roi_multiplier:.2f}x")
        print(f"   - Strategic alignment: {report.strategic_alignment_score:.3f}")
        
        print(f"\nüìä Predictive Intelligence:")
        if report.predicted_insight_quality:
            print(f"   Quality predictions for patterns:")
            for pattern_name, quality in list(report.predicted_insight_quality.items())[:3]:
                print(f"   - {pattern_name}: {quality:.3f}")
        
        if report.recommended_investigation_areas:
            print(f"\n   Recommended investigation areas:")
            for area in report.recommended_investigation_areas[:3]:
                print(f"   - {area}")
        
        print(f"\nüìä Optimization Opportunities:")
        if report.optimization_opportunities:
            print(f"   Found {len(report.optimization_opportunities)} optimizations:")
            for opt in report.optimization_opportunities[:2]:
                print(f"   - {opt['area']}: {opt['recommendation']}")
                print(f"     Expected improvement: {opt['expected_improvement']:.2%}")
        
        print("\n‚úÖ Comprehensive intelligence report test completed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Intelligence report test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run all Phase 2.3 cross-module intelligence tests."""
    print("üöÄ Investigation-Insight Cross-Module Intelligence Test Suite - Phase 2.3")
    print("=" * 80)
    
    tests = [
        ("Engine Initialization", test_engine_initialization),
        ("Investigation-Insight Linking", test_investigation_insight_linking),
        ("Pattern Discovery", test_pattern_discovery),
        ("Feedback Loops", test_feedback_loops),
        ("Predictive Capabilities", test_predictive_capabilities),
        ("Pipeline Optimization", test_pipeline_optimization),
        ("Comprehensive Intelligence", test_comprehensive_intelligence_report)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = await test_func()
        except Exception as e:
            print(f"\n‚ùå {test_name} crashed: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 80)
    print("üìä PHASE 2.3 TEST SUMMARY")
    print("=" * 80)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{test_name}: {status}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\nüéâ ALL PHASE 2.3 TESTS PASSED - Cross-Module Intelligence Complete!")
        print("\nüìã Phase 2.3 Achievements:")
        print("   ‚úÖ Investigation-Insight link discovery and analysis")
        print("   ‚úÖ Cross-module pattern recognition and learning")
        print("   ‚úÖ Bidirectional feedback loop establishment")
        print("   ‚úÖ Predictive insight quality estimation")
        print("   ‚úÖ Investigation area recommendations")
        print("   ‚úÖ Pipeline optimization analysis")
        print("   ‚úÖ Comprehensive intelligence reporting")
        print("   ‚úÖ ROI and business value calculation")
        print("   ‚úÖ Strategic alignment assessment")
        print("   ‚úÖ Performance monitoring and improvement tracking")
        print("\nüèÜ LanceDB-Centric Ecosystem Integration Complete!")
        print("\nüìä Summary of Completed Phases:")
        print("   Phase 0: Vector Infrastructure Foundation ‚úÖ")
        print("   Phase 1: Intelligence Module Integration ‚úÖ")
        print("   Phase 2: Investigation & Insight Synthesis Integration ‚úÖ")
        print("   Phase 2.3: Cross-Module Intelligence ‚úÖ")
        print("\nüöÄ Ready for production deployment with full vector intelligence!")
        return 0
    else:
        print("\n‚ö†Ô∏è Some Phase 2.3 tests failed")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)